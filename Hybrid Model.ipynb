{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dhritiwasan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dhritiwasan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "ps = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'grow', 'move', 'winding', 'loved', 'million', 'flower', 'bloom', 'working', 'truly', 'gem', 'marching', 'said', 'still', 'guiding', 'flow', 'home', 'get', 'vision', 'twinkling', 'dot', 'moment', 'build', 'fall', 'sunrise', 'striving', 'mind', 'echoing', 'united', 'bright', 'thus', 'soul', 'fly', 'feeling', 'exicted', 'heat', 'starting', 'passed', 'different', 'thousand', 'youll', 'seems', 'unfold', 'long', 'pretty', 'want', 'think', 'one', 'toil', 'add', 'dark', 'white', 'reaching', 'catch', 'road', 'conviction', 'star', 'rainbow', 'strive', 'recall', 'yearning', 'especially', 'prepared', 'darkest', 'bus', 'hollywood', 'air', 'surprise', 'grown', 'riverside', 'take', 'sweet', 'democratic', 'turning', 'built', 'beginning', 'let', 'join', 'knew', 'hand', 'carried', 'refreshed', 'near', 'greenery', 'stranger', 'open', 'surely', 'low', 'come', 'live', 'core', 'high', 'overcome', 'francisco', 'quay', 'spot', 'history', 'sound', 'tame', 'world', 'truth', 'belongs', 'beautiful', 'beat', 'flame', 'loud', 'back', 'calling', 'anew', 'great', 'lion', 'snow', 'river', 'lead', 'blood', 'comfort', 'ago', 'trip', 'isle', 'afar', 'dare', 'san', 'taste', 'must', 'walked', 'aspire', 'become', 'inspiration', 'highest', 'choose', 'made', 'struggled', 'stirred', 'cold', 'ring', 'west', 'thought', 'towards', 'change', 'scale', 'walking', 'wish', 'close', 'alive', 'sign', 'unsure', 'unforgettable', 'feel', 'life', 'looking', 'big', 'citizen', 'begin', 'guide', 'spirit', 'people', 'voice', 'future', 'equality', 'name', 'flight', 'beyond', 'head', 'regardless', 'bravely', 'forward', 'away', 'like', 'higher', 'smile', 'see', 'magic', 'limit', 'dear', 'ben', 'cairo', 'spain', 'faraway', 'tower', 'cross', 'beauty', 'compare', 'street', 'try', 'memory', 'best', 'sour', 'fine', 'swim', 'seven', 'happy', 'language', 'test', 'wake', 'creating', 'meet', 'blend', 'society', 'remain', 'undivided', 'tall', 'whether', 'standing', 'prosperity', 'shore', 'whenever', 'hear', 'common', 'living', 'however', 'ocean', 'always', 'everywhere', 'started', 'wan', 'kept', 'challenge', 'right', 'top', 'way', 'gon', 'wide', 'easy', 'discovered', 'leap', 'enchanting', 'homeland', 'sing', 'journey', 'someday', 'perfect', 'morning', 'light', 'finally', 'five', 'look', 'across', 'happen', 'inside', 'role', 'wall', 'help', 'alone', 'sight', 'trust', 'play', 'roll', 'raise', 'difference', 'cared', 'something', 'kwai', 'whole', 'belong', 'awaits', 'lie', 'seen', 'forget', 'celebrate', 'bird', 'born', 'together', 'sharing', 'believing', 'climb', 'far', 'call', 'could', 'cause', 'island', 'broadway', 'stage', 'everyone', 'heart', 'tell', 'grace', 'step', 'precious', 'hour', 'peace', 'singing', 'stop', 'pledge', 'accustomed', 'side', 'touch', 'care', 'knowledge', 'afraid', 'dream', 'bay', 'know', 'rocky', 'city', 'carry', 'okay', 'warms', 'progress', 'believe', 'dreaming', 'tomorrow', 'grew', 'part', 'fire', 'sure', 'drum', 'turn', 'love', 'grand', 'moving', 'elsewhere', 'rest', 'chorus', 'seem', 'show', 'good', 'anything', 'tie', 'deep', 'strength', 'glorious', 'yeah', 'may', 'bind', 'tranquil', 'story', 'young', 'safe', 'renewed', 'brightly', 'youthful', 'danced', 'closest', 'singaporean', 'experience', 'gather', 'began', 'song', 'horizon', 'opened', 'neon', 'jewel', 'forevermore', 'weather', 'face', 'sunset', 'plain', 'soar', 'ever', 'justice', 'miss', 'flag', 'going', 'around', 'friend', 'first', 'stride', 'race', 'distance', 'stood', 'merry', 'find', 'brand', 'write', 'alll', 'seize', 'time', 'bearing', 'tiny', 'current', 'give', 'country', 'brighter', 'goal', 'hold', 'true', 'special', 'everything', 'london', 'better', 'within', 'braved', 'town', 'glory', 'amazing', 'land', 'shout', 'crossed', 'seemed', 'greets', 'keep', 'religion', 'make', 'crescent', 'youre', 'today', 'done', 'lamp', 'bang', 'roam', 'creed', 'count', 'determination', 'shine', 'roamed', 'free', 'destiny', 'recess', 'even', 'wind', 'never', 'bitter', 'happiness', 'share', 'height', 'rather', 'along', 'brave', 'sun', 'bridge', 'mountain', 'shining', 'wherever', 'found', 'raffle', 'sea', 'bring', 'heartbeat', 'wonderful', 'crimson', 'many', 'mankind', 'freedom', 'really', 'building', 'nowhere', 'stand', 'moved', 'hundred', 'spring', 'night', 'laughter', 'choice', 'told', 'closer', 'queued', 'ready', 'based', 'forever', 'every', 'bell', 'rain', 'darkness', 'color', 'gone', 'real', 'little', 'climbed', 'remember', 'tried', 'renew', 'matter', 'delightful', 'experienced', 'faith', 'harmony', 'achieve', 'sail', 'thing', 'stormy', 'stay', 'put', 'stepping', 'hope', 'win', 'dozen', 'singapore', 'else', 'place', 'worthwhile', 'shared', 'imagine', 'start', 'contribute', 'peaceful', 'reality', 'upon', 'run', 'strong', 'others', 'got', 'stronger', 'old', 'child', 'arising', 'till', 'treasure', 'climbing', 'moon', 'fellow', 'lively', 'eye', 'wanted', 'pure', 'storm', 'wait', 'bold', 'year', 'reach', 'ahead', 'yet', 'day', 'sens', 'book', 'courage', 'flying', 'family', 'nation', 'nothing', 'joy', 'learn', 'much', 'yes', 'bombay', 'roar', 'proud', 'wing', 'corner', 'man', 'esplanade', 'heard', 'set', 'vigilance', 'lighting', 'end', 'walk', 'seeing', 'warm', 'brings', 'possible', 'trouble', 'sky', 'vast', 'sunny', 'new'}\n"
     ]
    }
   ],
   "source": [
    "words = set()\n",
    "\n",
    "for f in sorted(os.listdir('songs')):\n",
    "    if f[0] == '.':\n",
    "        continue\n",
    "    \n",
    "    for i, l in enumerate(open('songs/' + f, 'r').readlines()):\n",
    "        \n",
    "        if i < 2:\n",
    "            continue\n",
    "        \n",
    "        arr = word_tokenize(l)\n",
    "        for a in arr:\n",
    "            a = lemma.lemmatize(a.lower())\n",
    "            if \"'\" in a:\n",
    "                continue\n",
    "            if len(a) < 3:\n",
    "                continue\n",
    "            if not a in model.vocab:\n",
    "                continue\n",
    "            if a in stop_words:\n",
    "                continue\n",
    "            words.add(a)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "centroids = np.zeros((len(words), 300))\n",
    "\n",
    "for i, w in enumerate(words):\n",
    "    centroids[i] = model[w]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.zeros((len(words), len(words)))\n",
    "for i in range(dist.shape[0]):\n",
    "    for j in range(dist.shape[1]):\n",
    "        dist[i, j] = np.linalg.norm(centroids[i] - centroids[j])\n",
    "out = open('dist.txt', 'w')\n",
    "for i in range(dist.shape[0]):\n",
    "    for j in range(dist.shape[1]):\n",
    "        out.write(str(dist[i, j]).ljust(20) + ' ')\n",
    "    out.write('\\n')\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(544, 544)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(centroids[:,2], centroids[:,3], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=50).fit(centroids)\n",
    "labels = gmm.predict(centroids)\n",
    "mapping = {}\n",
    "for s, l in sorted(zip(words, labels), key = lambda x: x[1]):\n",
    "    mapping[s] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 1. 0. 1.]\n",
      " [0. 1. 1. ... 0. 0. 1.]\n",
      " [0. 1. 1. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 1. 1. ... 0. 1. 1.]\n",
      " [0. 1. 1. ... 0. 0. 1.]\n",
      " [0. 1. 1. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "songs = sorted(os.listdir('songs'))\n",
    "mfw = np.zeros((len(songs), 50))\n",
    "\n",
    "for index, f in enumerate(songs):\n",
    "    if f[0] == '.':\n",
    "        continue\n",
    "    \n",
    "    for i, l in enumerate(open('songs/' + f, 'r').readlines()):\n",
    "        \n",
    "        if i < 2:\n",
    "            continue\n",
    "        \n",
    "        arr = word_tokenize(l)\n",
    "        for a in arr:\n",
    "            a = lemma.lemmatize(a.lower())\n",
    "            if \"'\" in a:\n",
    "                continue\n",
    "            if len(a) < 3:\n",
    "                continue\n",
    "            if not a in model.vocab:\n",
    "                continue\n",
    "            if a in stop_words:\n",
    "                continue\n",
    "            mfw[index, mapping[a]] = 1\n",
    "            \n",
    "print(mfw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.zeros((len(songs), len(songs)))\n",
    "for i in range(dist.shape[0]):\n",
    "    for j in range(dist.shape[1]):\n",
    "        dist[i, j] = np.linalg.norm(mfw[i] - mfw[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 29)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007.txt 0\n",
      "1962.txt 1\n",
      "1967.txt 1\n",
      "1969.txt 1\n",
      "1999.txt 1\n",
      "2000.txt 1\n",
      "2001.txt 1\n",
      "2002.txt 1\n",
      "2004.txt 1\n",
      "2006.txt 1\n",
      "2007-2.txt 1\n",
      "2012.txt 1\n",
      "2015.txt 1\n",
      "1984.txt 2\n",
      "1986.txt 2\n",
      "1990.txt 2\n",
      "1991.txt 2\n",
      "2003.txt 2\n",
      "2009.txt 2\n",
      "2013.txt 2\n",
      "2017.txt 2\n",
      "1997.txt 3\n",
      "1999-2.txt 3\n",
      "2005.txt 3\n",
      "2008.txt 3\n",
      "2010.txt 3\n",
      "2011.txt 3\n",
      "2016.txt 3\n",
      "1987.txt 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=5).fit(mfw)\n",
    "labels = gmm.predict(mfw)\n",
    "for s, l in sorted(zip(songs, labels), key = lambda x: x[1]):\n",
    "    print(s.ljust(8),l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-42afacc15da0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmfw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "print(mfw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
